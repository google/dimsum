// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang -std=c++11 -S -O2 -mavx2 -o - %s | FileCheck -check-prefix=CHECK-AVX2 %s
#include "dimsum.h"
using namespace dimsum;

extern "C" {

// CHECK-AVX2-LABEL: operatorADDEQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddb (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddw (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddd (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddq (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddb (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddw (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddd (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpaddq (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_8:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vaddps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_8(NativeSimd<float>& a, NativeSimd<float> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorADDEQ_9:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vaddpd (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovapd %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorADDEQ_9(NativeSimd<double>& a, NativeSimd<double> b) {
  a += b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubb %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubw %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubb %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubw %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsubq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_8:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovaps (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vsubps %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_8(NativeSimd<float>& a, NativeSimd<float> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorSUBEQ_9:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovapd (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vsubpd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovapd %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSUBEQ_9(NativeSimd<double>& a, NativeSimd<double> b) {
  a -= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm2
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm2, %ymm2
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm3
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm3, %ymm3
// CHECK-AVX2-NEXT:    vpmullw %ymm2, %ymm3, %ymm2
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm2, %xmm3
// CHECK-AVX2-NEXT:    vmovdqa {{.*#+}} xmm4 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm3, %xmm3
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm2, %xmm2
// CHECK-AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm2 = xmm2[0],xmm3[0]
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm0, %ymm0
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm1, %ymm1
// CHECK-AVX2-NEXT:    vpmullw %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm1
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm1, %xmm1
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm0, %xmm0
// CHECK-AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm0[0],xmm1[0]
// CHECK-AVX2-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpmullw (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpmulld (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsrlq $32, %ymm0, %ymm2
// CHECK-AVX2-NEXT:    vpmuludq %ymm2, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpsrlq $32, %ymm1, %ymm3
// CHECK-AVX2-NEXT:    vpmuludq %ymm0, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpaddq %ymm3, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpsllq $32, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpmuludq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpaddq %ymm2, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm2
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm2, %ymm2
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm3
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm3, %ymm3
// CHECK-AVX2-NEXT:    vpmullw %ymm2, %ymm3, %ymm2
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm2, %xmm3
// CHECK-AVX2-NEXT:    vmovdqa {{.*#+}} xmm4 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm3, %xmm3
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm2, %xmm2
// CHECK-AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm2 = xmm2[0],xmm3[0]
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm0, %ymm0
// CHECK-AVX2-NEXT:    vpmovsxbw %xmm1, %ymm1
// CHECK-AVX2-NEXT:    vpmullw %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm1
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm1, %xmm1
// CHECK-AVX2-NEXT:    vpshufb %xmm4, %xmm0, %xmm0
// CHECK-AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm0[0],xmm1[0]
// CHECK-AVX2-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpmullw (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpmulld (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsrlq $32, %ymm0, %ymm2
// CHECK-AVX2-NEXT:    vpmuludq %ymm2, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpsrlq $32, %ymm1, %ymm3
// CHECK-AVX2-NEXT:    vpmuludq %ymm0, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpaddq %ymm3, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpsllq $32, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpmuludq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpaddq %ymm2, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_8:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmulps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_8(NativeSimd<float>& a, NativeSimd<float> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorMULEQ_9:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmulpd (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovapd %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorMULEQ_9(NativeSimd<double>& a, NativeSimd<double> b) {
  a *= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllw $5, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpsllw $4, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsllw $2, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpaddb %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpaddb %ymm1, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpaddb %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm3 = ymm0[4],ymm2[4],ymm0[5],ymm2[5],ymm0[6],ymm2[6],ymm0[7],ymm2[7],ymm0[12],ymm2[12],ymm0[13],ymm2[13],ymm0[14],ymm2[14],ymm0[15],ymm2[15]
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm4 = ymm2[4],ymm1[4],ymm2[5],ymm1[5],ymm2[6],ymm1[6],ymm2[7],ymm1[7],ymm2[12],ymm1[12],ymm2[13],ymm1[13],ymm2[14],ymm1[14],ymm2[15],ymm1[15]
// CHECK-AVX2-NEXT:    vpsllvd %ymm3, %ymm4, %ymm3
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm0 = ymm0[0],ymm2[0],ymm0[1],ymm2[1],ymm0[2],ymm2[2],ymm0[3],ymm2[3],ymm0[8],ymm2[8],ymm0[9],ymm2[9],ymm0[10],ymm2[10],ymm0[11],ymm2[11]
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm1 = ymm2[0],ymm1[0],ymm2[1],ymm1[1],ymm2[2],ymm1[2],ymm2[3],ymm1[3],ymm2[8],ymm1[8],ymm2[9],ymm1[9],ymm2[10],ymm1[10],ymm2[11],ymm1[11]
// CHECK-AVX2-NEXT:    vpsllvd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpackusdw %ymm3, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllvd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllvq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllw $5, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpsllw $4, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsllw $2, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpaddb %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpaddb %ymm1, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpaddb %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm3 = ymm0[4],ymm2[4],ymm0[5],ymm2[5],ymm0[6],ymm2[6],ymm0[7],ymm2[7],ymm0[12],ymm2[12],ymm0[13],ymm2[13],ymm0[14],ymm2[14],ymm0[15],ymm2[15]
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm4 = ymm2[4],ymm1[4],ymm2[5],ymm1[5],ymm2[6],ymm1[6],ymm2[7],ymm1[7],ymm2[12],ymm1[12],ymm2[13],ymm1[13],ymm2[14],ymm1[14],ymm2[15],ymm1[15]
// CHECK-AVX2-NEXT:    vpsllvd %ymm3, %ymm4, %ymm3
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm0 = ymm0[0],ymm2[0],ymm0[1],ymm2[1],ymm0[2],ymm2[2],ymm0[3],ymm2[3],ymm0[8],ymm2[8],ymm0[9],ymm2[9],ymm0[10],ymm2[10],ymm0[11],ymm2[11]
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm1 = ymm2[0],ymm1[0],ymm2[1],ymm1[1],ymm2[2],ymm1[2],ymm2[3],ymm1[3],ymm2[8],ymm1[8],ymm2[9],ymm1[9],ymm2[10],ymm1[10],ymm2[11],ymm1[11]
// CHECK-AVX2-NEXT:    vpsllvd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpackusdw %ymm3, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllvd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHLEQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllvq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHLEQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a <<= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllw $5, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpunpckhbw {{.*#+}} ymm2 = ymm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,24,24,25,25,26,26,27,27,28,28,29,29,30,30,31,31]
// CHECK-AVX2-NEXT:    vpunpckhbw {{.*#+}} ymm3 = ymm0[8],ymm1[8],ymm0[9],ymm1[9],ymm0[10],ymm1[10],ymm0[11],ymm1[11],ymm0[12],ymm1[12],ymm0[13],ymm1[13],ymm0[14],ymm1[14],ymm0[15],ymm1[15],ymm0[24],ymm1[24],ymm0[25],ymm1[25],ymm0[26],ymm1[26],ymm0[27],ymm1[27],ymm0[28],ymm1[28],ymm0[29],ymm1[29],ymm0[30],ymm1[30],ymm0[31],ymm1[31]
// CHECK-AVX2-NEXT:    vpsraw $4, %ymm3, %ymm4
// CHECK-AVX2-NEXT:    vpblendvb %ymm2, %ymm4, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpsraw $2, %ymm3, %ymm4
// CHECK-AVX2-NEXT:    vpaddw %ymm2, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpblendvb %ymm2, %ymm4, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpsraw $1, %ymm3, %ymm4
// CHECK-AVX2-NEXT:    vpaddw %ymm2, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpblendvb %ymm2, %ymm4, %ymm3, %ymm2
// CHECK-AVX2-NEXT:    vpsrlw $8, %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpunpcklbw {{.*#+}} ymm0 = ymm0[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23]
// CHECK-AVX2-NEXT:    vpunpcklbw {{.*#+}} ymm1 = ymm0[0],ymm1[0],ymm0[1],ymm1[1],ymm0[2],ymm1[2],ymm0[3],ymm1[3],ymm0[4],ymm1[4],ymm0[5],ymm1[5],ymm0[6],ymm1[6],ymm0[7],ymm1[7],ymm0[16],ymm1[16],ymm0[17],ymm1[17],ymm0[18],ymm1[18],ymm0[19],ymm1[19],ymm0[20],ymm1[20],ymm0[21],ymm1[21],ymm0[22],ymm1[22],ymm0[23],ymm1[23]
// CHECK-AVX2-NEXT:    vpsraw $4, %ymm1, %ymm3
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm3, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsraw $2, %ymm1, %ymm3
// CHECK-AVX2-NEXT:    vpaddw %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm3, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsraw $1, %ymm1, %ymm3
// CHECK-AVX2-NEXT:    vpaddw %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm3, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpsrlw $8, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpackuswb %ymm2, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm3 = ymm0[4],ymm2[4],ymm0[5],ymm2[5],ymm0[6],ymm2[6],ymm0[7],ymm2[7],ymm0[12],ymm2[12],ymm0[13],ymm2[13],ymm0[14],ymm2[14],ymm0[15],ymm2[15]
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm4 = ymm2[4],ymm1[4],ymm2[5],ymm1[5],ymm2[6],ymm1[6],ymm2[7],ymm1[7],ymm2[12],ymm1[12],ymm2[13],ymm1[13],ymm2[14],ymm1[14],ymm2[15],ymm1[15]
// CHECK-AVX2-NEXT:    vpsravd %ymm3, %ymm4, %ymm3
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm0 = ymm0[0],ymm2[0],ymm0[1],ymm2[1],ymm0[2],ymm2[2],ymm0[3],ymm2[3],ymm0[8],ymm2[8],ymm0[9],ymm2[9],ymm0[10],ymm2[10],ymm0[11],ymm2[11]
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm1 = ymm2[0],ymm1[0],ymm2[1],ymm1[1],ymm2[2],ymm1[2],ymm2[3],ymm1[3],ymm2[8],ymm1[8],ymm2[9],ymm1[9],ymm2[10],ymm1[10],ymm2[11],ymm1[11]
// CHECK-AVX2-NEXT:    vpsravd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpackusdw %ymm3, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsravd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vpbroadcastq {{.*#+}} ymm1 = [9223372036854775808,9223372036854775808,9223372036854775808,9223372036854775808]
// CHECK-AVX2-NEXT:    vpsrlvq %ymm0, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpxor (%rdi), %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsrlvq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpsubq %ymm2, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsllw $5, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpsrlw $4, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsrlw $2, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpaddb %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm1
// CHECK-AVX2-NEXT:    vpsrlw $1, %ymm1, %ymm2
// CHECK-AVX2-NEXT:    vpand {{.*}}(%rip), %ymm2, %ymm2
// CHECK-AVX2-NEXT:    vpaddb %ymm0, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpblendvb %ymm0, %ymm2, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm3 = ymm0[4],ymm2[4],ymm0[5],ymm2[5],ymm0[6],ymm2[6],ymm0[7],ymm2[7],ymm0[12],ymm2[12],ymm0[13],ymm2[13],ymm0[14],ymm2[14],ymm0[15],ymm2[15]
// CHECK-AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm4 = ymm2[4],ymm1[4],ymm2[5],ymm1[5],ymm2[6],ymm1[6],ymm2[7],ymm1[7],ymm2[12],ymm1[12],ymm2[13],ymm1[13],ymm2[14],ymm1[14],ymm2[15],ymm1[15]
// CHECK-AVX2-NEXT:    vpsrlvd %ymm3, %ymm4, %ymm3
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm3, %ymm3
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm0 = ymm0[0],ymm2[0],ymm0[1],ymm2[1],ymm0[2],ymm2[2],ymm0[3],ymm2[3],ymm0[8],ymm2[8],ymm0[9],ymm2[9],ymm0[10],ymm2[10],ymm0[11],ymm2[11]
// CHECK-AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm1 = ymm2[0],ymm1[0],ymm2[1],ymm1[1],ymm2[2],ymm1[2],ymm2[3],ymm1[3],ymm2[8],ymm1[8],ymm2[9],ymm1[9],ymm2[10],ymm1[10],ymm2[11],ymm1[11]
// CHECK-AVX2-NEXT:    vpsrlvd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vpsrld $16, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vpackusdw %ymm3, %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsrlvd %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorSHREQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vmovdqa (%rdi), %ymm1
// CHECK-AVX2-NEXT:    vpsrlvq %ymm0, %ymm1, %ymm0
// CHECK-AVX2-NEXT:    vmovdqa %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorSHREQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a >>= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorANDEQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vandps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorANDEQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a &= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorXOREQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vxorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorXOREQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a ^= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_0:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_0(NativeSimd<int8>& a, NativeSimd<int8> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_1:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_1(NativeSimd<int16>& a, NativeSimd<int16> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_2:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_2(NativeSimd<int32>& a, NativeSimd<int32> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_3:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_3(NativeSimd<int64>& a, NativeSimd<int64> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_4:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_4(NativeSimd<uint8>& a, NativeSimd<uint8> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_5:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_5(NativeSimd<uint16>& a, NativeSimd<uint16> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_6:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_6(NativeSimd<uint32>& a, NativeSimd<uint32> b) {
  a |= b;
}

// CHECK-AVX2-LABEL: operatorOREQ_7:
// CHECK-AVX2:       # %bb.0: # %entry
// CHECK-AVX2-NEXT:    vorps (%rdi), %ymm0, %ymm0
// CHECK-AVX2-NEXT:    vmovaps %ymm0, (%rdi)
// CHECK-AVX2-NEXT:    vzeroupper
// CHECK-AVX2-NEXT:    retq
void operatorOREQ_7(NativeSimd<uint64>& a, NativeSimd<uint64> b) {
  a |= b;
}

}
